{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a buisness std data frame to concate with attributes and hours on a normalized json\n",
    "#the normalized json will then dissect the dictionary into its emperical form\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "business = pd.read_json('yelp_academic_dataset_business.json', lines =True)\n",
    "business_attributes = pd.json_normalize(business['attributes'])\n",
    "business_hours = pd.json_normalize(business['hours'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use result as a dummy for a sequential conocating\n",
    "#keep buisness on the left for formating purposes\n",
    "#axis = 1 because we dont need another index\n",
    "result = pd.concat([business, business_attributes], axis=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see above\n",
    "business = pd.concat([result, business_hours], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refer to project one on explanation of this code\n",
    "#it drops the cateogries and split like a .explode function except it creates duplicates by expanding whenever , is present. \n",
    "#restack\n",
    "#reset index and rename it after adding it in\n",
    "business_data = business.drop('categories', axis=1).join(business['categories'].str.split(',', expand=True).stack().reset_index(level=1, drop=True).rename('establishment_type'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set this as type str for regex prep\n",
    "business_data['establishment_type'] = business_data['establishment_type'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Restaurants            0.055299\n",
       " Food                   0.031401\n",
       " Shopping               0.028286\n",
       "Restaurants             0.022865\n",
       " Home Services          0.015796\n",
       "                          ...   \n",
       " Patent Law             0.000001\n",
       "LAN Centers             0.000001\n",
       "Holistic Animal Care    0.000001\n",
       "Bocce Ball              0.000001\n",
       " Trade Fairs            0.000001\n",
       "Name: establishment_type, Length: 2455, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#acknowledge resturants is # 1 \n",
    "business_data.establishment_type.value_counts(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re, find the data that only contains resturants and food with no case sensitivity \n",
    "import re \n",
    "business_dataprep = business_data[business_data['establishment_type'].str.contains('.*Restaurants.*')| business_data['establishment_type'].str.contains('.*Food.*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicate ids because its the same resturant \n",
    "buisness_data_prep = business_dataprep.drop_duplicates(subset=['business_id'], ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain the data from philly only\n",
    "geo_example_philly= buisness_data_prep_copy[buisness_data_prep_copy['city'] == 'Philadelphia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n"
     ]
    }
   ],
   "source": [
    "#now we incorporate NLTK text merge \n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.width', 10000)\n",
    "\n",
    "# open input file: \n",
    "ifile = open('yelp_academic_dataset_review-001.json', encoding = 'ascii', errors = 'ignore') \n",
    "\n",
    "# read the first 100k entries\n",
    "# set to -1 to process everything\n",
    "stop = 600000\n",
    "\n",
    "all_data = list()\n",
    "for i, line in enumerate(ifile):\n",
    "    if i%10000==0:\n",
    "        print(i)\n",
    "    if i==stop:\n",
    "        break    \n",
    "    # convert the json on this line to a dict\n",
    "    data = json.loads(line)\n",
    "    # extract what we want\n",
    "    text = data['text']\n",
    "    stars = data['stars']\n",
    "    user_id = data['user_id']\n",
    "    business_id= data[\"business_id\"]\n",
    "    # add to the data collected so far\n",
    "    all_data.append([stars, text, user_id, business_id])\n",
    "# create the DataFrame\n",
    "yelp_reviews_raw = pd.DataFrame(all_data, columns=['stars','text', 'user_id', 'business_id'])\n",
    "\n",
    "# df.to_hdf('"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_philly_merge= pd.merge(geo_example_philly, yelp_reviews_raw, on='business_id', how =\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_philly_merge.to_csv('EDA.csv')\n",
    "days = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "37fa83a94d701a151562ebbdb9be3785de780639a788d264f4ef77025d1fd793"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
